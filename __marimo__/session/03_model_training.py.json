{
  "version": "1",
  "metadata": {
    "marimo_version": "0.18.0"
  },
  "cells": [
    {
      "id": "MJUe",
      "code_hash": "a934b1fd568eb05d057289971f3c772e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h1 id=\"module-3-model-training-experimentation\">Module 3: Model Training &amp; Experimentation</h1>\n<span class=\"paragraph\"><strong>\"Think in experiments, not models\"</strong> \u2014 Every successful ML team</span>\n<span class=\"paragraph\">Welcome to the most exciting part: training models! But here's the key insight:\n<strong>Success in ML comes from systematic experimentation, not finding the \"best\" algorithm.</strong></span>\n<h2 id=\"what-youll-learn\">What You'll Learn</h2>\n<ol>\n<li><strong>Experimentation Mindset</strong>: Formulate hypotheses and test systematically</li>\n<li><strong>The Model Zoo</strong>: When to use linear models vs trees vs neural networks</li>\n<li><strong>Cross-Validation</strong>: Get robust performance estimates</li>\n<li><strong>Hyperparameter Tuning</strong>: Optimize models without overfitting</li>\n<li><strong>Experiment Tracking</strong>: Never lose track of what works</li>\n</ol>\n<h2 id=\"industry-reality\">Industry Reality</h2>\n<blockquote>\n<span class=\"paragraph\">\"We ran 10,000+ experiments to improve our model by 2%\"\n\u2014 Google Brain team</span>\n</blockquote>\n<span class=\"paragraph\"><strong>Why systematic experimentation matters:</strong></span>\n<ul>\n<li>No single \"best\" algorithm (it depends on your data)</li>\n<li>Small improvements compound to big business value</li>\n<li>Documentation prevents repeating mistakes</li>\n<li>Reproducibility is critical for debugging</li>\n</ul>\n<span class=\"paragraph\">Let's learn to experiment like a pro!</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "f2e497f53841cc9bd2508124e816ed99",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"section-1-data-splitting-the-foundation\">Section 1: Data Splitting (The Foundation)</h2>\n<span class=\"paragraph\"><strong>Critical question</strong>: How do we know if our model is any good?</span>\n<span class=\"paragraph\">Answer: We need to test it on data it hasn't seen during training!</span>\n<h3 id=\"the-standard-split\">The Standard Split</h3>\n<ul>\n<li><strong>Training set</strong> (70%): Model learns from this</li>\n<li><strong>Validation set</strong> (15%): Tune hyperparameters, compare models</li>\n<li><strong>Test set</strong> (15%): Final evaluation (touch once!)</li>\n</ul>\n<span class=\"paragraph\"><strong>Why 3 sets?</strong></span>\n<ul>\n<li>Training: For learning</li>\n<li>Validation: For model selection (prevents overfitting to test set!)</li>\n<li>Test: Unbiased final performance estimate</li>\n</ul>\n<span class=\"paragraph\">Let's split our data properly:</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "33f63b3b11b0902c77cc114b29be487b",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><hr />\n<h2 id=\"section-2-baseline-model-always-start-here\">Section 2: Baseline Model (Always Start Here!)</h2>\n<span class=\"paragraph\"><strong>Rule #1 of ML</strong>: Always start with a simple baseline.</span>\n<span class=\"paragraph\"><strong>Why?</strong></span>\n<ul>\n<li>Know if your problem is even solvable</li>\n<li>Understand the difficulty</li>\n<li>Have something to compare against</li>\n<li>Catch data issues early</li>\n</ul>\n<span class=\"paragraph\"><strong>Common baselines</strong>:</span>\n<ul>\n<li><strong>Dummy classifier</strong>: Always predict most common class</li>\n<li><strong>Simple model</strong>: Logistic Regression or Decision Tree</li>\n</ul>\n<span class=\"paragraph\">Let's establish our baseline:</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "1c6c9177a1cd67d1e9eb3cef9500e72b",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"baseline-insights\">\ud83d\udca1 Baseline Insights</h3>\n<span class=\"paragraph\"><strong>Key observations</strong>:</span>\n<ul>\n<li>Dummy classifier: ~15-20% (just guessing most common type)</li>\n<li>Logistic Regression: ~50-60% (learning real patterns!)</li>\n</ul>\n<span class=\"paragraph\"><strong>What this tells us</strong>:</span>\n<ul>\n<li>Problem is definitely solvable with ML</li>\n<li>Features contain useful signal</li>\n<li>But there's room for improvement (60% \u2192 80%+?)</li>\n</ul>\n<span class=\"paragraph\">This is realistic! Most ML projects start here.</span>\n<hr />\n<h2 id=\"section-3-the-model-zoo-which-model-when\">Section 3: The Model Zoo (Which Model When?)</h2>\n<span class=\"paragraph\"><strong>Key insight</strong>: No free lunch theorem says \"no algorithm is best for all problems.\"</span>\n<span class=\"paragraph\">So how do we choose? Let's train multiple model types and compare!</span>\n<h3 id=\"models-well-try\">Models We'll Try</h3>\n<ol>\n<li><strong>Logistic Regression</strong> (linear, fast, interpretable)</li>\n<li><strong>Decision Tree</strong> (non-linear, interpretable, baseline for trees)</li>\n<li><strong>Random Forest</strong> (ensemble of trees, robust, good default)</li>\n<li><strong>Gradient Boosting (XGBoost)</strong> (often best for tabular data)</li>\n</ol>\n<span class=\"paragraph\">Let's train all four and compare systematically:</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "790df344b277b9f61df196e29754b64e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"cross-validation-insights\">\ud83d\udca1 Cross-Validation Insights</h3>\n<span class=\"paragraph\"><strong>Why standard deviation matters</strong>:</span>\n<ul>\n<li>Low std: Model is stable across different data splits</li>\n<li>High std: Model performance varies (might be overfitting or data issues)</li>\n</ul>\n<span class=\"paragraph\"><strong>When to use CV</strong>:</span>\n<ul>\n<li>\u2705 Model selection (which algorithm?)</li>\n<li>\u2705 Feature selection (which features?)</li>\n<li>\u2705 Hyperparameter tuning (which settings?)</li>\n<li>\u274c Final test evaluation (use held-out test set once!)</li>\n</ul>\n<hr />\n<h2 id=\"section-5-hyperparameter-tuning\">Section 5: Hyperparameter Tuning</h2>\n<span class=\"paragraph\"><strong>Hyperparameters</strong> = Settings you choose before training</span>\n<span class=\"paragraph\">Examples:</span>\n<ul>\n<li>Random Forest: n_estimators, max_depth, min_samples_split</li>\n<li>XGBoost: learning_rate, max_depth, n_estimators</li>\n<li>Neural Networks: learning_rate, batch_size, hidden_layers</li>\n</ul>\n<span class=\"paragraph\"><strong>Challenge</strong>: How do we find good hyperparameters?</span>\n<h3 id=\"approaches\">Approaches:</h3>\n<ol>\n<li><strong>Manual tuning</strong>: Try values one by one (slow!)</li>\n<li><strong>Grid Search</strong>: Try all combinations (thorough but expensive)</li>\n<li><strong>Random Search</strong>: Try random combinations (faster, often good enough)</li>\n<li><strong>Bayesian Optimization</strong>: Smart search (advanced)</li>\n</ol>\n<span class=\"paragraph\">Let's use Grid Search on Random Forest:</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Pvdt",
      "code_hash": "9a9f68d2c4ade4eb9e725167b7f465d9",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"hyperparameter-tuning-insights\">\ud83c\udfaf Hyperparameter Tuning Insights</h3>\n<span class=\"paragraph\"><strong>Key learnings</strong>:</span>\n<ul>\n<li>Tuning can improve performance by 2-5% (significant!)</li>\n<li>But it's expensive (trying many combinations)</li>\n<li>Diminishing returns after a point</li>\n</ul>\n<span class=\"paragraph\"><strong>Best practices</strong>:</span>\n<ol>\n<li>Start with default hyperparameters</li>\n<li>Understand which hyperparameters matter most</li>\n<li>Use Random Search first (faster)</li>\n<li>Use Grid Search to refine</li>\n<li>Always validate on held-out data</li>\n</ol>\n<h3 id=\"common-pitfall-hyperparameter-overfitting\">\u26a0\ufe0f Common Pitfall: Hyperparameter Overfitting</h3>\n<span class=\"paragraph\">If you tune hyperparameters too aggressively on validation set,\nyou overfit to the validation set!</span>\n<span class=\"paragraph\"><strong>Solution</strong>: Use nested CV or keep a true test set untouched.</span>\n<hr />\n<h2 id=\"section-6-experiment-tracking\">Section 6: Experiment Tracking</h2>\n<span class=\"paragraph\"><strong>Problem</strong>: You've run 10 experiments. Which one was best? What settings did you use?</span>\n<span class=\"paragraph\"><strong>Solution</strong>: Track everything!</span>\n<span class=\"paragraph\">In production, use tools like:</span>\n<ul>\n<li><strong>MLflow</strong>: Open source experiment tracking</li>\n<li><strong>Weights &amp; Biases</strong>: Cloud-based with rich visualizations</li>\n<li><strong>Neptune</strong>: Experiment management platform</li>\n</ul>\n<span class=\"paragraph\">For this project, we'll use a simple approach:</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "f2f48be8ac2f4e494898cd4cd2e507f2",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"why-experiment-tracking-matters\">\ud83d\udcca Why Experiment Tracking Matters</h3>\n<span class=\"paragraph\"><strong>In production</strong>:</span>\n<ul>\n<li>You'll run 100s or 1000s of experiments</li>\n<li>Need to compare across weeks/months</li>\n<li>Team members need to see what worked</li>\n<li>Reproducibility is critical</li>\n</ul>\n<span class=\"paragraph\"><strong>What to track</strong>:</span>\n<ul>\n<li>\u2705 Model type and hyperparameters</li>\n<li>\u2705 Training and validation metrics</li>\n<li>\u2705 Training time and resources</li>\n<li>\u2705 Feature set used</li>\n<li>\u2705 Data version</li>\n<li>\u2705 Code version (git commit)</li>\n<li>\u2705 Random seeds</li>\n<li>\u2705 Notes and observations</li>\n</ul>\n<hr />\n<h2 id=\"section-7-key-takeaways-socratic-questions\">Section 7: Key Takeaways &amp; Socratic Questions</h2>\n<h3 id=\"what-you-learned\">\u2705 What You Learned</h3>\n<ol>\n<li><strong>Always start with a baseline</strong> (know your starting point)</li>\n<li><strong>Try multiple model types</strong> (no free lunch theorem)</li>\n<li><strong>Use cross-validation</strong> (robust performance estimates)</li>\n<li><strong>Tune hyperparameters systematically</strong> (grid/random search)</li>\n<li><strong>Track everything</strong> (experiments, not just final models)</li>\n</ol>\n<h3 id=\"socratic-questions\">\ud83e\udd14 Socratic Questions</h3>\n<ol>\n<li>\n<span class=\"paragraph\"><strong>\"Your validation accuracy is 90% but test accuracy is 70%. What happened?\"</strong></span>\n<ul>\n<li>Answer: Overfit to validation set through tuning</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>\"When would you choose Logistic Regression over XGBoost, even if XGBoost is more accurate?\"</strong></span>\n<ul>\n<li>Think: Interpretability, speed, production constraints, model size</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>\"You're tuning hyperparameters. Should you use test data to select the best model? Why not?\"</strong></span>\n<ul>\n<li>Answer: No! Test set must remain untouched until final evaluation</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>\"Your model is 90% accurate but your PM is unhappy. What might you be missing?\"</strong></span>\n<ul>\n<li>Think: Class imbalance, cost-sensitive errors, business metrics vs ML metrics</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>\"Training takes 8 hours. How do you experiment efficiently?\"</strong></span>\n<ul>\n<li>Think: Smaller data samples, simpler models first, distributed training, caching</li>\n</ul>\n</li>\n</ol>\n<hr />\n<h2 id=\"industry-context\">\ud83c\udfe2 Industry Context</h2>\n<h3 id=\"how-companies-do-this-at-scale\">How Companies Do This at Scale</h3>\n<span class=\"paragraph\"><strong>Netflix</strong>:</span>\n<ul>\n<li>1000s of experiments tracked in MLflow</li>\n<li>Automated hyperparameter tuning (Optuna)</li>\n<li>A/B tests for final model selection</li>\n<li>Retraining pipelines (Airflow)</li>\n</ul>\n<span class=\"paragraph\"><strong>Google</strong>:</span>\n<ul>\n<li>AutoML for hyperparameter search</li>\n<li>Distributed training (multiple GPUs/TPUs)</li>\n<li>Experiment management (internal tools)</li>\n<li>Rigorous A/B testing</li>\n</ul>\n<span class=\"paragraph\"><strong>Small Startup</strong>:</span>\n<ul>\n<li>Manual tracking (spreadsheets initially)</li>\n<li>Grid/random search for tuning</li>\n<li>Simple retraining scripts</li>\n<li>Focus on business metrics</li>\n</ul>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<span class=\"paragraph\">\u26a0\ufe0f <strong>Don't</strong>: Train one model and call it done\n\u2705 <strong>Do</strong>: Try multiple approaches systematically</span>\n<span class=\"paragraph\">\u26a0\ufe0f <strong>Don't</strong>: Tune hyperparameters on test set\n\u2705 <strong>Do</strong>: Use validation set or CV for tuning</span>\n<span class=\"paragraph\">\u26a0\ufe0f <strong>Don't</strong>: Forget to track experiments\n\u2705 <strong>Do</strong>: Log everything from day one</span>\n<span class=\"paragraph\">\u26a0\ufe0f <strong>Don't</strong>: Overfit through excessive tuning\n\u2705 <strong>Do</strong>: Use nested CV or keep test set sacred</span>\n<hr />\n<h2 id=\"module-3-checkpoint\">\ud83c\udfaf Module 3 Checkpoint</h2>\n<span class=\"paragraph\">You've completed Module 3 when you can:</span>\n<ul class=\"task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Set up an experiment from scratch in 15 minutes</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Explain bias-variance tradeoff with concrete example</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Choose appropriate model for a new problem</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Run cross-validation and interpret results</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Tune hyperparameters without overfitting</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled/> Track experiments systematically</li>\n</ul>\n<span class=\"paragraph\"><strong>Next</strong>: Module 4 - Model Evaluation &amp; Validation</span>\n<span class=\"paragraph\">In the next module, you'll learn how to:</span>\n<ul>\n<li>Evaluate beyond accuracy</li>\n<li>Analyze errors systematically</li>\n<li>Create model cards</li>\n<li>Communicate model quality to stakeholders</li>\n</ul></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Hbol",
      "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e",
      "outputs": [],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "a7ab88ee2d14ae30147670519209348f",
      "outputs": [],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "562f17d0e4ae501ef233aece61824525",
      "outputs": [],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "6d91fe96fc9c3bc14daf2448f85b6b19",
      "outputs": [],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "c0416d8f27a9b309ad2ecf49c704e1b4",
      "outputs": [],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "2fba3e29ffc3ada88afcf22c225a90d1",
      "outputs": [],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "39d108c09df28b858ebb2b987b6cf17d",
      "outputs": [],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "1a7351b7dcf848c6fb463372abb7a540",
      "outputs": [],
      "console": []
    },
    {
      "id": "Hstk",
      "code_hash": "4b402966d35598fddec93aa81ba27268",
      "outputs": [],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "016fd9e30c853f142d94694f3d3ba7c3",
      "outputs": [],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "da3008c4b27cd6af312ff62521474f20",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "5b9299a5d7ab2c7e8e00d1c10af336af",
      "outputs": [],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "26bc4e77985c2ee9b7c0d047b8a67ea5",
      "outputs": [],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "9bf7c63d9d0cea832d52b8889c13e3d6",
      "outputs": [],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "3b8cada6863ff43c9b40b065681f7ddb",
      "outputs": [],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "3c74094301941c2cca529085a81c8623",
      "outputs": [],
      "console": []
    },
    {
      "id": "ulZA",
      "code_hash": "dce1bb4923023122c7edfd37df7d4b33",
      "outputs": [],
      "console": []
    },
    {
      "id": "ecfG",
      "code_hash": "8e9a7ae54b19aa7ed7c4624468a5b6fd",
      "outputs": [],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "1fee71cf5126672c230b46e4e898f880",
      "outputs": [],
      "console": []
    }
  ]
}