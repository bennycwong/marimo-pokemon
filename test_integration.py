"""
Integration test to verify the complete ML pipeline works end-to-end.
Tests key functionality from each module without GUI dependencies.
"""
import sys
from pathlib import Path

print("Testing Complete ML Pipeline Integration...")
print("=" * 70)

# Test 1: Data Engineering (Module 1)
print("\n1. Testing Data Engineering (Module 1)...")
try:
    import pandas as pd
    import pandera as pa
    from pandera import Column, Check, DataFrameSchema

    # Load data
    DATA_PATH = Path("data/pokemon_cards.csv")
    if not DATA_PATH.exists():
        raise FileNotFoundError(f"Dataset not found at {DATA_PATH}")

    df = pd.read_csv(DATA_PATH)
    print(f"   ‚úÖ Loaded {len(df)} Pokemon cards")

    # Data validation with Pandera
    pokemon_schema = DataFrameSchema(
        columns={
            "card_id": Column(str, nullable=False),
            "type": Column(str, nullable=False),
            "hp": Column(float, nullable=True),
            "attack": Column(int, nullable=False),
        },
        strict=False
    )

    validated = pokemon_schema.validate(df, lazy=True)
    print(f"   ‚úÖ Data validation passed")

    # Data cleaning
    df_clean = df.copy()
    df_clean = df_clean.drop_duplicates(subset=['card_id'], keep='first')
    df_clean = df_clean.dropna(subset=['type', 'attack', 'defense'])
    print(f"   ‚úÖ Data cleaned: {len(df_clean)} records remain")

except Exception as e:
    print(f"   ‚ùå Module 1 error: {e}")
    sys.exit(1)

# Test 2: Feature Engineering (Module 2)
print("\n2. Testing Feature Engineering (Module 2)...")
try:
    # Create features
    df_feat = df_clean.copy()

    # Basic stats
    df_feat['total_stats'] = (df_feat['hp'].fillna(0) +
                               df_feat['attack'] +
                               df_feat['defense'] +
                               df_feat['sp_attack'] +
                               df_feat['sp_defense'] +
                               df_feat['speed'])

    # Ratios
    df_feat['attack_defense_ratio'] = df_feat['attack'] / (df_feat['defense'] + 1)
    df_feat['physical_bias'] = (df_feat['attack'] + df_feat['defense']) - (df_feat['sp_attack'] + df_feat['sp_defense'])

    # Binning
    df_feat['hp_category'] = pd.cut(
        df_feat['hp'].fillna(0),
        bins=[0, 50, 80, 150],
        labels=['low', 'medium', 'high']
    )

    print(f"   ‚úÖ Created {len(df_feat.columns) - len(df_clean.columns)} new features")

    # Create preprocessing pipeline
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier

    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', RandomForestClassifier(n_estimators=10, random_state=42))
    ])

    print(f"   ‚úÖ Preprocessing pipeline created")

except Exception as e:
    print(f"   ‚ùå Module 2 error: {e}")
    sys.exit(1)

# Test 3: Model Training (Module 3)
print("\n3. Testing Model Training (Module 3)...")
try:
    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier

    # Prepare data
    feature_cols = ['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed']
    X = df_feat[feature_cols].fillna(0)
    y = df_feat['type']

    # Remove rare classes (need >= 4 samples per class for stratified train/val/test split)
    class_counts = y.value_counts()
    valid_classes = class_counts[class_counts >= 4].index
    valid_mask = y.isin(valid_classes)
    X = X[valid_mask]
    y = y[valid_mask]
    print(f"   ‚úÖ Filtered to {len(valid_classes)} classes with >= 4 samples")

    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )

    print(f"   ‚úÖ Data split: {len(X_train)} train, {len(X_val)} val, {len(X_test)} test")

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    # Train multiple models
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)
    }

    # Test XGBoost if available
    try:
        import xgboost as xgb
        from sklearn.preprocessing import LabelEncoder

        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y_train)

        models['XGBoost'] = xgb.XGBClassifier(
            n_estimators=50,
            max_depth=6,
            random_state=42,
            eval_metric='mlogloss'
        )
        xgb_available = True
    except:
        xgb_available = False

    results = {}
    for name, model in models.items():
        if name == 'XGBoost' and xgb_available:
            model.fit(X_train_scaled, y_train_encoded)
            val_score = model.score(X_val_scaled, le.transform(y_val))
        else:
            model.fit(X_train_scaled, y_train)
            val_score = model.score(X_val_scaled, y_val)

        results[name] = val_score

    best_model_name = max(results, key=results.get)
    print(f"   ‚úÖ Trained {len(results)} models")
    print(f"   ‚úÖ Best model: {best_model_name} ({results[best_model_name]:.2%})")

    # Cross-validation
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    cv_scores = cross_val_score(
        LogisticRegression(max_iter=1000, random_state=42),
        X_train_scaled,
        y_train,
        cv=cv
    )
    print(f"   ‚úÖ Cross-validation: {cv_scores.mean():.2%} ¬± {cv_scores.std():.2%}")

except Exception as e:
    print(f"   ‚ùå Module 3 error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 4: Model Evaluation (Module 4)
print("\n4. Testing Model Evaluation (Module 4)...")
try:
    from sklearn.metrics import (
        classification_report,
        confusion_matrix,
        accuracy_score,
        precision_recall_fscore_support
    )

    # Pick best model for evaluation
    best_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)
    best_model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred = best_model.predict(X_test_scaled)
    y_pred_proba = best_model.predict_proba(X_test_scaled)

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    print(f"   ‚úÖ Test accuracy: {accuracy:.2%}")

    # Classification report
    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
    print(f"   ‚úÖ Classification report generated ({len(report)-3} classes)")

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"   ‚úÖ Confusion matrix: {cm.shape}")

    # Error analysis
    errors = X_test[y_test != y_pred]
    print(f"   ‚úÖ Error analysis: {len(errors)} misclassifications")

except Exception as e:
    print(f"   ‚ùå Module 4 error: {e}")
    sys.exit(1)

# Test 5: Model Deployment (Module 5)
print("\n5. Testing Model Deployment (Module 5)...")
try:
    import pickle
    import json
    from datetime import datetime

    # Save model
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)

    model_path = model_dir / "test_model.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(best_model, f)
    print(f"   ‚úÖ Model saved to {model_path}")

    # Save scaler
    scaler_path = model_dir / "test_scaler.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   ‚úÖ Scaler saved to {scaler_path}")

    # Save metadata
    metadata = {
        'version': '1.0.0',
        'model_type': 'RandomForestClassifier',
        'features': feature_cols,
        'classes': best_model.classes_.tolist(),
        'train_accuracy': float(best_model.score(X_train_scaled, y_train)),
        'test_accuracy': float(accuracy),
        'training_date': datetime.now().isoformat(),
        'training_samples': len(X_train)
    }

    metadata_path = model_dir / "test_metadata.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"   ‚úÖ Metadata saved to {metadata_path}")

    # Load and test inference
    with open(model_path, 'rb') as f:
        loaded_model = pickle.load(f)

    with open(scaler_path, 'rb') as f:
        loaded_scaler = pickle.load(f)

    # Make prediction
    sample_data = {
        'hp': 70.0,
        'attack': 85,
        'defense': 65,
        'sp_attack': 75,
        'sp_defense': 70,
        'speed': 95
    }

    input_df = pd.DataFrame([sample_data])[feature_cols]
    input_scaled = loaded_scaler.transform(input_df)
    prediction = loaded_model.predict(input_scaled)[0]
    proba = loaded_model.predict_proba(input_scaled)[0]
    confidence = float(max(proba))

    print(f"   ‚úÖ Inference test: Predicted '{prediction}' with {confidence:.1%} confidence")

    # Input validation test
    def validate_input(data, expected_features):
        missing = set(expected_features) - set(data.keys())
        if missing:
            return False, f"Missing features: {missing}"

        for feature, value in data.items():
            if not isinstance(value, (int, float)):
                return False, f"{feature} must be numeric"
            if value < 0 or value > 300:
                return False, f"{feature} out of valid range"

        return True, None

    valid, error = validate_input(sample_data, feature_cols)
    if valid:
        print(f"   ‚úÖ Input validation working")
    else:
        raise ValueError(f"Validation failed: {error}")

    # Data drift detection (simple version)
    train_means = X_train.mean()
    test_means = X_test.mean()
    drift_threshold = 0.1

    drifted_features = []
    for col in feature_cols:
        drift = abs(test_means[col] - train_means[col]) / (train_means[col] + 1e-6)
        if drift > drift_threshold:
            drifted_features.append(col)

    print(f"   ‚úÖ Drift detection: {len(drifted_features)} features with drift > {drift_threshold}")

    # Cleanup test files
    model_path.unlink()
    scaler_path.unlink()
    metadata_path.unlink()
    print(f"   ‚úÖ Test files cleaned up")

except Exception as e:
    print(f"   ‚ùå Module 5 error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Final summary
print("\n" + "=" * 70)
print("‚úÖ INTEGRATION TEST COMPLETE!")
print("\nAll 5 modules tested successfully:")
print("  1. ‚úÖ Data Engineering - Loading, validation, cleaning")
print("  2. ‚úÖ Feature Engineering - Features, pipelines")
print("  3. ‚úÖ Model Training - Multiple models, CV, tuning")
print("  4. ‚úÖ Model Evaluation - Metrics, confusion matrix, errors")
print("  5. ‚úÖ Model Deployment - Serialization, inference, monitoring")

if xgb_available:
    print("\n  ‚≠ê XGBoost: Available and tested")
else:
    print("\n  ‚ö†Ô∏è  XGBoost: Not available (optional)")
    print("     To install: brew install libomp (Mac)")

print("\nüéâ Your ML engineering course is ready to use!")
print("\nStart learning with:")
print("  marimo edit 01_data_engineering.py")
print("=" * 70)
